#The script shows how to join two columns using their names and dump it into hadoop storage on my AWS account 

register s3n://uw-cse-344-oregon.aws.amazon.com/myudfs.jar

#raw = LOAD 's3n://uw-cse-344-oregon.aws.amazon.com/cse344-test-file' USING TextLoader as (line:chararray);

raw = LOAD 's3n://uw-cse-344-oregon.aws.amazon.com/btc-2010-chunk-000' USING TextLoader as (line:chararray); 

ntriples = foreach raw generate FLATTEN(myudfs.RDFSplit3(line)) as (subject:chararray,predicate:chararray,object:chararray);


#filtered = filter ntriples by (subject matches '.*business.*') PARALLEL 50;

filtered = filter ntriples by (subject matches '.*rdfabout\\.com.*') PARALLEL 50;


filtered_copy = foreach filtered generate $0 as subject2, $1 as predicate2, $2 as object2;


#joined = JOIN filtered BY subject, filtered_copy BY subject2;
joined = JOIN filtered BY object, filtered_copy BY subject2;

##Remove duplicate tuples (Important in case of joins)
final_joined = DISTINCT joined;

#store joined into '/tmp/finaljoin' using PigStorage();

#store joined into '/user/hadoop/join-small' using PigStorage();

store final_joined into '/user/hadoop/join-big' using PigStorage();

